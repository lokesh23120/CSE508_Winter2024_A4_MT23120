{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pandas as pd\n","review_df=pd.read_csv('/kaggle/input/amz-review/Reviews.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Drop all columns except 'Summary' and 'Text'\n","review_df = review_df[['Summary', 'Text']]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(review_df.columns)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pandas as pd\n","from bs4 import BeautifulSoup\n","import re\n","from transformers import GPT2Tokenizer\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","\n","!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/\n","\n","# Initialize the WordNet Lemmatizer\n","lemmatizer = WordNetLemmatizer()\n","\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Preprocess text function with lemmatization and GPT2 tokenizer\n","def preprocess_text(text):\n","    # Check if text is not null and is a string\n","    if isinstance(text, str):\n","        # Remove HTML tags\n","        text = BeautifulSoup(text, \"html.parser\").get_text()\n","\n","        # Convert text to lowercase\n","        text = text.lower()\n","\n","        text = re.sub(r'[^\\w\\s]', '', text)\n","\n","        # Tokenization using GPT2 tokenizer\n","        tokens = tokenizer.tokenize(text)\n","\n","        # Perform lemmatization\n","        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n","\n","        # Join the tokens back into a single string and remove consecutive spaces\n","        processed_text = ' '.join(lemmatized_tokens).strip()\n","\n","        return processed_text\n","    else:\n","        return ''\n","\n","# Load or create the review_df dataframe here\n","\n","# Apply preprocessing function to 'Summary' and 'Text' columns and save them as new columns\n","review_df['Summary_Preprocessed'] = review_df['Summary'].apply(preprocess_text)\n","review_df['Text_Preprocessed'] = review_df['Text'].apply(preprocess_text)\n","\n","# Keep only the original and preprocessed summary and text columns\n","processed_df = review_df[['Summary', 'Summary_Preprocessed', 'Text', 'Text_Preprocessed']]\n","\n","# Save the preprocessed dataframe to a CSV file\n","processed_df.to_csv(\"/kaggle/working/PREPROCESSE_REVIEW.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pandas as pd\n","preprocessed_df=pd.read_csv('/kaggle/input/preprocessed-rev/PREPROCESSE_REVIEW.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(preprocessed_df.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(len(preprocessed_df))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["new_1L_df = preprocessed_df.iloc[50000:60000]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(len(new_1L_df))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# Perform train-test split with 75% training data and 25% testing data\n","train_df, test_df = train_test_split(new_1L_df, test_size=0.25, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Store the test dataset into a CSV file\n","test_df.to_csv('/kaggle/working/test.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["pip install rouge-score"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install --upgrade jax jaxlib"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Import necessary libraries\n","import torch\n","from transformers import GPT2LMHeadModel\n","\n","# Load the model class\n","model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n","\n","# Specify the file path from where you saved the model\n","model_load_path = \"/kaggle/input/model-final/model.pth\"\n","\n","# Load the model's state dictionary\n","model.load_state_dict(torch.load(model_load_path))\n","\n","# Put the model in evaluation mode\n","model.eval()\n","\n","print(\"Model loaded successfully from:\", model_load_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Import libraries\n","import pandas as pd\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW\n","from rouge_score import rouge_scorer\n","from tqdm import tqdm\n","\n","# Define the dataset class\n","class SummaryDataset(Dataset):\n","    def __init__(self, data_df, tokenizer, max_length, padding):\n","        self.data_df = data_df\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","        self.padding = padding\n","\n","    def __getitem__(self, idx):\n","        review_text = self.data_df.iloc[idx]['Text_Preprocessed']\n","        summary_text = self.data_df.iloc[idx]['Summary_Preprocessed']\n","\n","        inputs = self.tokenizer(review_text, truncation=True, max_length=self.max_length, padding=self.padding, return_tensors=\"pt\")\n","        if isinstance(summary_text, str):\n","            summary_text = [summary_text]  # Convert single summary to list\n","\n","        labels = self.tokenizer(summary_text, truncation=True, max_length=self.max_length, padding=False, return_tensors=\"pt\").input_ids.squeeze(0)\n","\n","        return {\n","            'input_ids': inputs['input_ids'].squeeze(0),\n","            'attention_mask': inputs['attention_mask'].squeeze(0),\n","            'labels': labels\n","        }\n","\n","    def __len__(self):\n","        return len(self.data_df)\n","\n","# Define function to collate batches\n","def collate_fn(batch):\n","    max_length = max(len(item['input_ids']) for item in batch)\n","    input_ids = []\n","    attention_mask = []\n","    labels = []\n","    for item in batch:\n","        padded_input_ids = torch.nn.functional.pad(item['input_ids'], (0, max_length - len(item['input_ids'])), value=tokenizer.pad_token_id)\n","        padded_attention_mask = torch.nn.functional.pad(item['attention_mask'], (0, max_length - len(item['attention_mask'])), value=0)\n","        input_ids.append(padded_input_ids)\n","        attention_mask.append(padded_attention_mask)\n","        padded_labels = torch.nn.functional.pad(item['labels'], (0, max_length - len(item['labels'])), value=-100)  # Use -100 as padding for CrossEntropyLoss\n","        labels.append(padded_labels)\n","    input_ids = torch.stack(input_ids)\n","    attention_mask = torch.stack(attention_mask)\n","    labels = torch.stack(labels)\n","    return {\n","        'input_ids': input_ids,\n","        'attention_mask': attention_mask,\n","        'labels': labels\n","    }\n","\n","# # Load data and preprocess\n","# train_df = pd.read_csv(\"/path/to/train_dataset.csv\")\n","# test_df = pd.read_csv(\"/path/to/test_dataset.csv\")\n","train_df.dropna(subset=['Text_Preprocessed', 'Summary_Preprocessed'], inplace=True)\n","test_df.dropna(subset=['Text_Preprocessed', 'Summary_Preprocessed'], inplace=True)\n","\n","# Initialize tokenizer and model\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", padding_side=\"left\")\n","tokenizer.pad_token = tokenizer.eos_token\n","model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n","\n","# Define hyperparameters\n","learning_rate = 1e-5\n","batch_size = 8\n","num_epochs = 10\n","max_length = 128\n","padding = True\n","\n","# Create datasets and data loaders\n","train_dataset = SummaryDataset(train_df, tokenizer, max_length, padding)\n","test_dataset = SummaryDataset(test_df, tokenizer, max_length, padding)\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n","\n","# Define optimizer and scheduler\n","optimizer = AdamW(model.parameters(), lr=learning_rate)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n","\n","# Training loop\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","model.train()\n","\n","for epoch in range(num_epochs):\n","    total_loss = 0\n","    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch')\n","\n","    for batch in progress_bar:\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","        progress_bar.set_postfix({'loss': loss.item()})\n","    \n","    avg_train_loss = total_loss / len(train_loader)\n","    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Train Loss: {avg_train_loss:.4f}\")\n","    scheduler.step()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model_save_path = \"/kaggle/working/final-fine-tuned-model\"  \n","torch.save(model.state_dict(), model_save_path)\n","\n","print(\"Model saved successfully at:\", model_save_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import necessary libraries\n","import torch\n","from transformers import GPT2LMHeadModel\n","\n","# Load the model class\n","model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n","\n","# Specify the file path from where you saved the model\n","model_load_path = \"/kaggle/input/gpt-fine-tuned-model\"\n","\n","# Load the model's state dictionary\n","model.load_state_dict(torch.load(model_load_path))\n","\n","# Put the model in evaluation mode\n","model.eval()\n","\n","print(\"Model loaded successfully from:\", model_load_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import logging\n","\n","# Set the logging level to ERROR to suppress messages below this level\n","logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# test_df=pd.read_csv('/kaggle/input/test-dataset-original/test_dataset_original.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Initialize a list to store generated summaries\n","generated_summaries = []\n","\n","# Set the model to evaluation mode\n","model.eval()\n","\n","# Iterate through the test dataset\n","with torch.no_grad():\n","    for batch in tqdm(test_loader, desc='Generating Summaries', unit='batch'):\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","\n","        # Generate summaries\n","        outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=max_length+1, num_beams=4, early_stopping=True)\n","        generated_summaries.extend([tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True) for output in outputs])\n","\n","# Add generated summaries to the test dataset DataFrame\n","test_df['Generated_Summary'] = generated_summaries[:len(test_df)]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(test_df.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Function to decode text and summary\n","def decode_text(text):\n","    return text.replace('Ġ', ' ')\n","\n","# Decoding 'Text' column\n","test_df['Decoded_Text'] = test_df['Text'].apply(decode_text)\n","\n","# Decoding 'Summary' column\n","test_df['Decoded_Summary'] = test_df['Summary'].apply(decode_text)\n","\n","# Decoding 'Generated_Summary' column\n","test_df['Decoded_Generated_Summary'] = test_df['Generated_Summary'].apply(decode_text)\n","\n","# Displaying the DataFrame with decoded columns\n","print(test_df.head())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from rouge_score import rouge_scorer\n","\n","# Initialize Rouge scorer\n","scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n","\n","# Initialize lists to store ROUGE scores\n","rouge1_scores = []\n","rouge2_scores = []\n","rougeL_scores = []\n","\n","# Iterate through each pair of generated summary and preprocessed summary\n","for generated_summary, preprocessed_summary in zip(test_df['Generated_Summary'], test_df['Summary_Preprocessed']):\n","    # Calculate ROUGE scores for each pair\n","    scores = scorer.score(generated_summary, preprocessed_summary)\n","    \n","    # Append individual ROUGE scores to respective lists\n","    rouge1_scores.append(scores['rouge1'].fmeasure)\n","    rouge2_scores.append(scores['rouge2'].fmeasure)\n","    rougeL_scores.append(scores['rougeL'].fmeasure)\n","\n","# Add ROUGE scores to the test dataset DataFrame\n","test_df['ROUGE-1'] = rouge1_scores\n","test_df['ROUGE-2'] = rouge2_scores\n","test_df['ROUGE-L'] = rougeL_scores"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(test_df.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Save the DataFrame to a CSV file\n","test_df.to_csv('/kaggle/working/test.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(test_df.head())"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T03:33:09.433054Z","iopub.status.busy":"2024-04-23T03:33:09.432667Z","iopub.status.idle":"2024-04-23T03:33:26.762138Z","shell.execute_reply":"2024-04-23T03:33:26.761180Z","shell.execute_reply.started":"2024-04-23T03:33:09.433023Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d2ca931fede649918b5bb691b7803d83","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9183b405cf744776940a331ca4d6d4ff","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"592f885428b54a22aabd6a2c773a01a9","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Model loaded successfully from: /kaggle/input/model-final/model.pth\n"]}],"source":["# Import necessary libraries\n","import torch\n","from transformers import GPT2LMHeadModel\n","\n","# Load the model class\n","model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n","\n","# Specify the file path from where you saved the model\n","model_load_path = \"/kaggle/input/model-final/model.pth\"\n","\n","# Load the model's state dictionary\n","model.load_state_dict(torch.load(model_load_path))\n","\n","# Pevaluation mode\n","model.eval()\n","\n","print(\"Model loaded successfully from:\", model_load_path)\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T03:34:30.997511Z","iopub.status.busy":"2024-04-23T03:34:30.996736Z","iopub.status.idle":"2024-04-23T03:34:47.815051Z","shell.execute_reply":"2024-04-23T03:34:47.813774Z","shell.execute_reply.started":"2024-04-23T03:34:30.997477Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting rouge-score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.4.0)\n","Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge-score) (3.2.4)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.26.4)\n","Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.16.0)\n","Building wheels for collected packages: rouge-score\n","  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=334d2d53dfb34021c8f3eb7799d1d2666bdbf87d5e90747f4631ba26e0613802\n","  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n","Successfully built rouge-score\n","Installing collected packages: rouge-score\n","Successfully installed rouge-score-0.1.2\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install rouge-score"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T04:03:47.710119Z","iopub.status.busy":"2024-04-23T04:03:47.709252Z","iopub.status.idle":"2024-04-23T04:04:30.408046Z","shell.execute_reply":"2024-04-23T04:04:30.406912Z","shell.execute_reply.started":"2024-04-23T04:03:47.710088Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"name":"stdout","output_type":"stream","text":["Enter the input text:  Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal.\n","Enter the reference summary:  Great taffy\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Both `max_new_tokens` (=100) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"name":"stdout","output_type":"stream","text":["Generated Summary: summarize: great  t aff  at  a  great  price    there  was  a  wide  assortment  of  y ummy  t aff    delivery  was  very  quick\n","ROUGE Scores: {'rouge1': Score(precision=0.5, recall=0.045454545454545456, fmeasure=0.08333333333333334), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.5, recall=0.045454545454545456, fmeasure=0.08333333333333334)}\n"]}],"source":["from rouge_score import rouge_scorer\n","import torch\n","import pandas as pd\n","from bs4 import BeautifulSoup\n","import re\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","from nltk.corpus import stopwords\n","\n","# Download NLTK stopwords list \n","nltk.download('stopwords')\n","\n","# Get the list of stopwords\n","stop_words = set(stopwords.words('english'))\n","\n","# Download NLTK resources \n","nltk.download('punkt')\n","nltk.download('wordnet')\n","\n","# Initialize the GPT-2 tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","\n","# Initialize the WordNet Lemmatizer\n","lemmatizer = WordNetLemmatizer()\n","\n","# Define the device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define a function to preprocess text\n","def preprocess_text(text):\n","    if isinstance(text, str):\n","        text = BeautifulSoup(text, \"html.parser\").get_text()\n","        text = text.lower()\n","        text = re.sub(r'[^\\w\\s]', '', text)\n","        tokens = tokenizer.tokenize(text)\n","        # Remove stopwords\n","        tokens = [token for token in tokens if token not in stop_words]\n","        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n","        processed_text = ' '.join(lemmatized_tokens).strip()\n","        return processed_text\n","    else:\n","        return ''\n","\n","# Define a function to generate summary\n","def generate_summary(review_text, model, tokenizer, device, max_length=128):\n","    preprocessed_review_text = preprocess_text(review_text)\n","    inputs = tokenizer.encode(\"summarize: \" + preprocessed_review_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n","    inputs = inputs.to(device)\n","    model.to(device)\n","    tokenizer.pad_token = tokenizer.eos_token\n","    padding_token_id = tokenizer.pad_token_id\n","    attention_mask = inputs.ne(padding_token_id)\n","    summary_ids = model.generate(inputs, max_length=max_length, length_penalty=1.0, num_beams=4, early_stopping=True, attention_mask=attention_mask, max_new_tokens=100)\n","    generated_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n","    return generated_summary[:max_length]\n","\n","# Define a function to calculate ROUGE score\n","def calculate_rouge_score(generated_summary, actual_given_summary):\n","    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n","    scores = scorer.score(generated_summary, actual_given_summary)\n","    return scores\n","\n","# Take input text and reference summary from the user\n","input_text = input(\"Enter the input text: \")\n","actual_given_summary = input(\"Enter the reference summary: \")\n","\n","# Generate summary\n","generated_summary = generate_summary(input_text, model, tokenizer, device)\n","\n","# Decode the summary\n","decoded_summary = tokenizer.decode(tokenizer.encode(generated_summary), skip_special_tokens=True)\n","\n","# Calculate ROUGE scores\n","rouge_scores = calculate_rouge_score(generated_summary, actual_given_summary)\n","# Decode the summary to remove special tokens\n","decoded_generated_summary = generated_summary.replace(\"Ġ\", \" \")\n","\n","\n","print(\"Generated Summary:\", decoded_generated_summary)\n","print(\"ROUGE Scores:\", rouge_scores)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4856915,"sourceId":8199003,"sourceType":"datasetVersion"},{"datasetId":4856930,"sourceId":8199030,"sourceType":"datasetVersion"},{"datasetId":4857010,"sourceId":8199159,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
